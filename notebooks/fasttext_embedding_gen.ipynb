{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloc = \"data\"\n",
    "carhacking = \"Car_Hacking_Challenge_Dataset_rev20Mar2021\"\n",
    "prelim = \"0_Preliminary\"\n",
    "training = \"0_Training\"\n",
    "filename_0 = \"Pre_train_D_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "prelim_train_dir = os.path.join(\"..\", dataloc, \"raw\", carhacking, prelim, training)\n",
    "csv0 = os.path.join(prelim_train_dir, filename_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\", \"d7\", \"d8\"]] = df.Data.str.split(\" \", expand=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d2\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d1\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d3\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d4\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d5\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d6\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d7\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d8\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Class\"]==\"Attack\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"d1_int\"] = df.apply(lambda x: 999 if x[\"d1\"] is None else int(x[\"d1\"], 16), axis=1)\n",
    "df[\"d2_int\"] = df.apply(lambda x: 999 if x[\"d2\"] is None else int(x[\"d2\"], 16), axis=1)\n",
    "df[\"d3_int\"] = df.apply(lambda x: 999 if x[\"d3\"] is None else int(x[\"d3\"], 16), axis=1)\n",
    "df[\"d4_int\"] = df.apply(lambda x: 999 if x[\"d4\"] is None else int(x[\"d4\"], 16), axis=1)\n",
    "\n",
    "df[\"d5_int\"] = df.apply(lambda x: 999 if x[\"d5\"] is None else int(x[\"d5\"], 16), axis=1)\n",
    "df[\"d6_int\"] = df.apply(lambda x: 999 if x[\"d6\"] is None else int(x[\"d6\"], 16), axis=1)\n",
    "df[\"d7_int\"] = df.apply(lambda x: 999 if x[\"d7\"] is None else int(x[\"d7\"], 16), axis=1)\n",
    "df[\"d8_int\"] = df.apply(lambda x: 999 if x[\"d8\"] is None else int(x[\"d8\"], 16), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"d7_int\"]==999].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"d1_int\", \"d2_int\", \"d3_int\", \"d4_int\", \"d5_int\", \"d6_int\", \"d7_int\", \"d8_int\"]].to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def build_nonoverlapping_sequence(X, seq_num=4):\n",
    "    seq_len = X.shape[1] * seq_num\n",
    "    n = math.floor(X.shape[0] / seq_len)\n",
    "    r = X.shape[0] % seq_len\n",
    "    if r != 0:\n",
    "        # Cut off not divisible part\n",
    "        seqs = X[:-r].reshape(-1,32)\n",
    "    else:\n",
    "        seqs = X.reshape(-1,32)\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.floor(X.shape[0] / (X.shape[1] * 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0] % 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
    "Xnew = build_nonoverlapping_sequence(X, 32)\n",
    "Xnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class UnsupervisedEmbedding(ABC):\n",
    "    def __init__(self, embedding_root: str = None, embedding_for: str = None,\n",
    "                 embedding_dim: int = 100, embedding_version: float = 1.0):\n",
    "        self.embedding_root = embedding_root\n",
    "        self.embedding_for = embedding_for\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_version = embedding_version\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_file_path(embedding_root: str = None, embedding_for: str = None,\n",
    "                             embedding_model='skipgram', embedding_wordNgrams: int = 1,\n",
    "                             embedding_dim: int = 100, train_seq_len: int = 10, embedding_version: float = 1.0,\n",
    "                             embedding_type='fasttext'):\n",
    "    filename = f\"{embedding_for}_{embedding_type}_{embedding_model}_\" \\\n",
    "               f\"{embedding_wordNgrams}wordNgram_{embedding_dim}dim_{train_seq_len}trainseq_v{embedding_version}.bin\"\n",
    "    print(os.path.join(embedding_root, filename))\n",
    "    return os.path.join(embedding_root, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEmbedding(UnsupervisedEmbedding):\n",
    "    def __init__(self, embedding_root: str = None, embedding_for: str = None,\n",
    "                 embedding_model='skipgram', embedding_wordNgrams: int = 1,\n",
    "                 embedding_dim: int = 100, embedding_version: float = 1.0,\n",
    "                 epochs: int = 10, minCount: int = 1, maxn: int = 0):\n",
    "        super().__init__(embedding_root, embedding_for, embedding_dim, embedding_version)\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "        # Can use wordN grams by setting 2\n",
    "        # https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "        self.embedding_wordNgrams = embedding_wordNgrams\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.minCount = minCount\n",
    "        self.maxn = maxn\n",
    "\n",
    "        self.embedding_config = {\n",
    "            'embedding_root': self.embedding_root,\n",
    "            'embedding_for': self.embedding_for, 'embedding_model': self.embedding_model,\n",
    "            \"embedding_wordNgrams\": self.embedding_wordNgrams, 'embedding_dim': self.embedding_dim,\n",
    "            'embedding_version': self.embedding_version, 'embedding_type': 'fasttext'\n",
    "        }\n",
    "\n",
    "    def fit(self, X):\n",
    "        seq_len = X.shape[1]\n",
    "        # if X is not None:\n",
    "        #     seq_len = len(_tokenize_by_spaces(X[0])) \n",
    "        # print(f\"Calc BBBBBBBBBB {seq_len} , X.shape[1] = {X.shape[1]}\")\n",
    "\n",
    "\n",
    "        data_temp_file_path = FastTextEmbedding.generate_temp_seq_storage_file_path(self.embedding_for)\n",
    "        np.savetxt(data_temp_file_path, X.astype(int), fmt='%i')\n",
    "\n",
    "        # Create embeddings for event id https://fasttext.cc/docs/en/python-module.html\n",
    "        fasttext_model = fasttext.train_unsupervised(data_temp_file_path,\n",
    "                                                     model=self.embedding_model,\n",
    "                                                     dim=self.embedding_dim,\n",
    "                                                     wordNgrams=self.embedding_wordNgrams,\n",
    "                                                     epoch=self.epochs, minCount=self.minCount, maxn=self.maxn)\n",
    "        cfg_copy = self.embedding_config.copy()\n",
    "        cfg_copy[\"train_seq_len\"] = seq_len\n",
    "        model_file_path = generate_model_file_path(**cfg_copy)\n",
    "        fasttext_model.save_model(model_file_path)\n",
    "\n",
    "        os.remove(data_temp_file_path)\n",
    "\n",
    "        # print(fasttext_model.get_words())\n",
    "        # word_embeddings = model.get_output_matrix()\n",
    "        # print(word_embeddings)\n",
    "\n",
    "        return fasttext_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_temp_seq_storage_file_path(embedding_for=None):\n",
    "        return embedding_for + '_eventid_token_seq.txt'"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fasttext_model.words)"
   ]
  },
  {
=======
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_embedding_cfg = {\n",
    "    'embedding_root': \"C:/Users/jvana/carhacking/features\",\n",
    "    'embedding_for': \"Car_Hacking_Challenge_Dataset_rev20Mar2021\", 'embedding_model': \"skipgram\",\n",
    "    \"embedding_wordNgrams\": 3, 'embedding_dim': 100,\n",
    "    'embedding_version': \"1.0\"\n",
    "}\n",
    "\n",
    "fasttext_embedding = FastTextEmbedding(**fastText_embedding_cfg)\n",
    "fasttext_model = fasttext_embedding.fit(Xnew) # fits and saves model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text_from_model_file(embedding_root: str = None, embedding_for: str = None,\n",
    "                              embedding_model='skipgram', embedding_wordNgrams: int = 1,\n",
    "                              embedding_dim: int = 100, train_seq_len: int = 10, embedding_version: float = 1.0):\n",
    "    model_file_path = generate_model_file_path(embedding_root, embedding_for, embedding_model,\n",
    "                                               embedding_wordNgrams, embedding_dim, train_seq_len,\n",
    "                                               embedding_version, 'fasttext')\n",
    "    model = fasttext.load_model(model_file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_embedding_cfg = {\n",
    "    'embedding_root': \"C:/Users/jvana/carhacking/features\",\n",
    "    'embedding_for': \"Car_Hacking_Challenge_Dataset_rev20Mar2021\", 'embedding_model': \"skipgram\",\n",
    "    \"embedding_wordNgrams\": 3, 'embedding_dim': 100,\n",
    "    'embedding_version': \"1.0\", \"train_seq_len\": 32\n",
    "}\n",
    "\n",
    "fasttext_model = fast_text_from_model_file(**fastText_embedding_cfg)\n",
    "word_embeddings = np.array([fasttext_model.get_word_vector(str(word_token))\n",
    "                            for word_token in np.arange(0,255)])\n",
    "\n",
    "#TODO: Extract normalized embeddings and do DBSCAN clustering"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = fasttext_model.get_word_vector(str(1))\n",
    "words"
=======
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "words = fasttext_model.get_word_vector(str(2))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_embeddings = word_embeddings/ np.linalg.norm(word_embeddings)\n",
    "normalized_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "eps = 0.5  # Adjust this value\n",
    "min_samples = 5  # Adjust this value\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(normalized_word_embeddings)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_embeddings)"
=======
    "word_embeddings"
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file in word2vec format. This is compact and we dont need fancy fasttext sub word token embeddings that bloats its propreitory feature storage in bin file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word2vec_model_file_path(embedding_root: str = None, embedding_for: str = None,\n",
    "                             embedding_model='skipgram', embedding_wordNgrams: int = 1,\n",
    "                             embedding_dim: int = 100, train_seq_len: int = 10, embedding_version: float = 1.0,\n",
    "                             embedding_type='fasttext'):\n",
    "    filename = f\"{embedding_for}_{embedding_type}_{embedding_model}_\" \\\n",
    "               f\"{embedding_wordNgrams}wordNgram_{embedding_dim}dim_{train_seq_len}trainseq_v{embedding_version}.word2vec\"\n",
    "    return os.path.join(embedding_root, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "fastText_embedding_cfg = {\n",
    "    'embedding_root': \"C:/Users/jvana/carhacking/features\",\n",
    "    'embedding_for': \"Car_Hacking_Challenge_Dataset_rev20Mar2021\", 'embedding_model': \"skipgram\",\n",
    "    \"embedding_wordNgrams\": 3, 'embedding_dim': 100,\n",
    "    'embedding_version': \"1.0\", \"train_seq_len\": 32\n",
    "}\n",
    "\n",
    "model_file_path = generate_word2vec_model_file_path(**fastText_embedding_cfg)\n",
    "\n",
    "#wv = KeyedVectors.load_word2vec_format(model_file_path)\n",
    "#wv_eventids = wv.index_to_key\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "wv = KeyedVectors.load_word2vec_format(model_file_path)"
   ]
=======
   "source": []
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "model_file_path"
   ]
=======
   "source": []
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> parent of 12b1d9b (added code to save, load word2vec)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carhacking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
