{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from logrca import extract_app_config\n",
    "from config.resolvers import LogRCA_ArtifactLocalBucketResolver\n",
    "\n",
    "from config.logrca_config import LogRCAConfig, LogFileConfig\n",
    "from logfile_utils import LogFileLocator\n",
    "from preprocessing.nltk_preproc import NltkTextPreprocessor\n",
    "from feature_extraction.tfidf import DataFrameTfIdfTransformer, DataFrameWord2VecPersister\n",
    "from feature_extraction.glove import GloveFeatureExtractor\n",
    "from feature_extraction.fasttext import FastTextFeatureExtractor, FastTextFeatureLoader\n",
    "from vulogrca_core.pipeline_utils import grid_parameters\n",
    "from feature_extraction.utils import cross_validated_feature_extraction\n",
    "from feature_extraction.fasttext_utils import write_word2vec, generate_word2vec_model_file_path\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrca_cfg, logfile_cfgs = extract_app_config(\n",
    "    inilocation=sys.path[1], logconfigjson=\"logs.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process config\n",
    "bucket_resolver = LogRCA_ArtifactLocalBucketResolver(logrca_cfg)\n",
    "bucket_resolver.create_nonexisting_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nltk_processing(log_locator):\n",
    "    df_unique_inliers = pd.read_csv(log_locator.get_unique_inliers_csv_path())\n",
    "    \n",
    "    nltk_proc = NltkTextPreprocessor(cols_to_preprocess=['event_template'], \n",
    "                                     append_preprocessed_cols=True, preprocessed_col_names=None)\n",
    "    df_nltk = nltk_proc.fit_transform(df_unique_inliers)\n",
    "    \n",
    "    # write back the pre-processed templates data into the same file\n",
    "    df_nltk.to_csv(log_locator.get_tfidf_csv_path(), index=False)\n",
    "    df_nltk = pd.read_csv(log_locator.get_tfidf_csv_path())\n",
    "\n",
    "    return df_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tfidf_features(logfile_cfg: LogFileConfig, df_nltk: pd.DataFrame):\n",
    "    logvector_persist_pipeline = Pipeline(steps=[\n",
    "        ('df_2_tfidf_transformer', DataFrameTfIdfTransformer()),\n",
    "        ('word2vec_persister', DataFrameWord2VecPersister())\n",
    "    ])\n",
    "\n",
    "    LOGVECTOR_PIPELINE_PARAMS = {\n",
    "        'df_2_tfidf_transformer__eventid_colname': logfile_cfg.drain[\"eventid_colname\"],\n",
    "        'df_2_tfidf_transformer__preprocessed_colname': 'event_template_preprocessed',\n",
    "        'df_2_tfidf_transformer__generate_dense_embeddings': True,\n",
    "        # we will decide this with custom cross validation\n",
    "        #'df_2_tfidf_transformer__feature_embedding_dim': 45, \n",
    "\n",
    "        'word2vec_persister__feature_root': bucket_resolver.featuresdir,\n",
    "        'word2vec_persister__feature_for': logfile_cfg.logfile_for,\n",
    "        'word2vec_persister__feature_version': logfile_cfg.version\n",
    "    }\n",
    "\n",
    "    # TODO: Use a tokenizer over preprocessed event templates to determine the vocab size.\n",
    "    # Or use a TFIDF vectorizer with dense = false, get the vectors and calculate sparsity\n",
    "    # Generally sparsity will be above 85-90 percent\n",
    "    # Rule of thumb: Upper limit for dense dimensions with above sparsity is 15-20% of max vocab size\n",
    "    cv_options = {\n",
    "        \"df_2_tfidf_transformer__feature_embedding_dim\": logfile_cfg.tfidf[\"feature_embedding_dims\"]\n",
    "    }\n",
    "    \n",
    "    explained_variances = {}\n",
    "    for cv_param in grid_parameters(cv_options):\n",
    "        pipeline_params = LOGVECTOR_PIPELINE_PARAMS | cv_param\n",
    "        # print(pipeline_params)\n",
    "\n",
    "        logvector_persist_pipeline.set_params(**pipeline_params)\n",
    "        logvector_persist_pipeline.fit_transform(df_nltk) # we dont care about the output\n",
    "\n",
    "        #v = logvector_persist_pipeline[\"df_2_tfidf_transformer\"]\n",
    "        tfidf_tx = logvector_persist_pipeline.named_steps.df_2_tfidf_transformer\n",
    "        explained_variance = tfidf_tx.explained_variance\n",
    "        dims = cv_param[\"df_2_tfidf_transformer__feature_embedding_dim\"]\n",
    "        explained_variances[dims] = explained_variance\n",
    "\n",
    "    return explained_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_glove_features(logfile_cfg: LogFileConfig, df_inliers: pd.DataFrame):\n",
    "    glove_feature_extraction_pipeline = Pipeline(steps=[\n",
    "        ('feature_extractor', GloveFeatureExtractor())\n",
    "    ])\n",
    "\n",
    "    GLOVE_FEATURE_EXTRACT_CFG = {\n",
    "        'feature_extractor__feature_for': logfile_cfg.logfile_for,\n",
    "        'feature_extractor__feature_root': bucket_resolver.featuresdir,\n",
    "        'feature_extractor__feature_version': logfile_cfg.version,\n",
    "        'feature_extractor__training_iterations': logfile_cfg.glove[\"training_iterations\"],\n",
    "        'feature_extractor__eventid_colname': logfile_cfg.drain[\"eventid_colname\"]\n",
    "        # 'feature_extractor__feature_embedding_dim': 25,\n",
    "        # 'feature_extractor__training_sequence_length': 50,\n",
    "        # 'feature_extractor__window_length': 5,\n",
    "    }\n",
    "\n",
    "    cv_options = {\n",
    "        \"feature_extractor__feature_embedding_dim\": logfile_cfg.glove[\"feature_embedding_dims\"],\n",
    "        \"feature_extractor__training_sequence_length\": logfile_cfg.glove[\"training_sequence_length\"],\n",
    "        'feature_extractor__window_length': logfile_cfg.glove[\"training_window_length\"]\n",
    "    }\n",
    "\n",
    "    cross_validated_feature_extraction(df=df_inliers, pipeline=glove_feature_extraction_pipeline, \n",
    "                                        pipeline_cfg=GLOVE_FEATURE_EXTRACT_CFG, cv_options=cv_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fastText_features(logfile_cfg: LogFileConfig, best_hyperparams: Dict[str, Any], df_inliers: pd.DataFrame):\n",
    "    fastText_feature_extraction_pipeline = Pipeline(steps=[\n",
    "        ('feature_extractor', FastTextFeatureExtractor(\n",
    "                                                    feature_for=logfile_cfg.logfile_for,\n",
    "                                                    feature_root=bucket_resolver.featuresdir,\n",
    "                                                    vocab_root=bucket_resolver.vocabdir,\n",
    "                                                    depth=best_hyperparams['depth'], st=best_hyperparams['st']) )\n",
    "    ])\n",
    "\n",
    "    FASTTEXT_FEATURE_EXTRACT_CFG = {\n",
    "        #'feature_extractor__feature_model': 'skipgram', # crossvalidation in future\n",
    "        #\"feature_extractor__feature_wordNgrams\": 3, # crossvalidation in future\n",
    "        #'feature_extractor__feature_embedding_dim': 32,\n",
    "        #'feature_extractor__training_sequence_length': 10,\n",
    "        'feature_extractor__feature_version': logfile_cfg.version,\n",
    "        'feature_extractor__vocab_version': logfile_cfg.version,\n",
    "        'feature_extractor__eventid_colname': logfile_cfg.drain[\"eventid_colname\"]\n",
    "    }\n",
    "\n",
    "    cv_options = {\n",
    "        'feature_extractor__feature_model': logfile_cfg.fastText[\"feature_model\"],\n",
    "        \"feature_extractor__feature_wordNgrams\": logfile_cfg.fastText[\"feature_wordNgrams\"],\n",
    "        \"feature_extractor__feature_embedding_dim\": logfile_cfg.fastText[\"feature_embedding_dims\"],\n",
    "        \"feature_extractor__training_sequence_length\": logfile_cfg.fastText[\"training_sequence_length\"] #putting a 5 here causes fasttext to get confused\n",
    "    }\n",
    "\n",
    "    cross_validated_feature_extraction(df=df_inliers, pipeline=fastText_feature_extraction_pipeline, \n",
    "                                        pipeline_cfg=FASTTEXT_FEATURE_EXTRACT_CFG, cv_options=cv_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastText_bin_model_2_word2vec(logfile_cfg: LogFileConfig, best_hyperparams: Dict[str, Any], df_fasttext: pd.DataFrame):\n",
    "    fasttext_2_word2vec_pipeline = Pipeline(steps=[\n",
    "    ('feature_loader', FastTextFeatureLoader(feature_for=logfile_cfg.logfile_for,\n",
    "                                       feature_root=bucket_resolver.featuresdir,\n",
    "                                       vocab_root=bucket_resolver.vocabdir,\n",
    "                                       feature_version=logfile_cfg.version, \n",
    "                                       vocab_version=logfile_cfg.version,\n",
    "                                       eventid_colname=logfile_cfg.drain[\"eventid_colname\"],\n",
    "                                       load_normalized_embeddings=False,\n",
    "                                       depth=best_hyperparams['depth'], st=best_hyperparams['st'])\n",
    "    )])\n",
    "\n",
    "    pipeline_cfg = {}\n",
    "    cv_options = {\n",
    "        'feature_loader__feature_model': logfile_cfg.fastText[\"feature_model\"],\n",
    "        \"feature_loader__feature_wordNgrams\": logfile_cfg.fastText[\"feature_wordNgrams\"],\n",
    "        \"feature_loader__feature_embedding_dim\": logfile_cfg.fastText[\"feature_embedding_dims\"],\n",
    "        \"feature_loader__training_sequence_length\":logfile_cfg.fastText[\"training_sequence_length\"] #putting a 5 here causes fasttext to get confused\n",
    "    }\n",
    "\n",
    "    for cv_param in grid_parameters(cv_options):\n",
    "        pipeline_params = pipeline_cfg | cv_param\n",
    "        fasttext_2_word2vec_pipeline.set_params(**pipeline_params)\n",
    "        fastText_embeddings = fasttext_2_word2vec_pipeline.fit_transform(df_fasttext)\n",
    "        #unique_event_id_tokens = fasttext_2_word2vec_pipeline.named_steps.feature_loader.unique_event_id_tokens\n",
    "        unique_event_ids = fasttext_2_word2vec_pipeline.named_steps.feature_loader.unique_event_ids\n",
    "        #print(unique_event_ids)\n",
    "        df = pd.DataFrame(data=fastText_embeddings, index=unique_event_ids)\n",
    "        w2cfilepath = generate_word2vec_model_file_path(\n",
    "                            embedding_root=bucket_resolver.featuresdir,\n",
    "                            embedding_for=logfile_cfg.logfile_for,\n",
    "                            embedding_model=cv_param['feature_loader__feature_model'], \n",
    "                            embedding_wordNgrams=cv_param['feature_loader__feature_wordNgrams'],\n",
    "                            embedding_dim=cv_param['feature_loader__feature_embedding_dim'], \n",
    "                            train_seq_len=cv_param['feature_loader__training_sequence_length'], \n",
    "                            embedding_version=logfile_cfg.version,\n",
    "                            embedding_type='fasttext')\n",
    "        if os.path.exists(w2cfilepath):\n",
    "            os.remove(w2cfilepath)\n",
    "        write_word2vec(df, w2cfilepath)\n",
    "\n",
    "    #return bin_loader.unique_event_id_tokens, fastText_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN MAIN\n",
    "for logfile_cfg in logfile_cfgs:\n",
    "    best_hyperparams = { \"depth\": 4, \"st\": 0.3}\n",
    "\n",
    "    log_locator = LogFileLocator(artifacts_root=logrca_cfg.general_config.artifactsroot, \n",
    "                                 logfile_for=logfile_cfg.logfile_for,\n",
    "                                 depth=best_hyperparams[\"depth\"],\n",
    "                                 similarity_threshold=best_hyperparams[\"st\"], \n",
    "                                 split_standard_deviation=logfile_cfg.drain[\"split_standard_deviations\"],\n",
    "                                 version=logfile_cfg.version)\n",
    "    df_nltk = apply_nltk_processing(log_locator)\n",
    "    explained_variances = extract_tfidf_features(logfile_cfg, df_nltk)\n",
    "\n",
    "    df_inliers = pd.read_csv(log_locator.get_inliers_csv_path())\n",
    "    df_unique_inliers = pd.read_csv(log_locator.get_unique_inliers_csv_path())\n",
    "    \n",
    "    extract_glove_features(logfile_cfg, df_inliers)\n",
    "    df_unique_inliers.to_csv(log_locator.get_spectral_csv_path(), index=False)\n",
    "    df_unique_inliers.to_csv(log_locator.get_normalized_glove_csv_path(), index=False)\n",
    "\n",
    "    # after extracting fast text features, convert fastText bin format word embeddings into word2vec\n",
    "    df_unique_inliers.to_csv(log_locator.get_fasttext_csv_path(), index=False)\n",
    "    extract_fastText_features(logfile_cfg, best_hyperparams, df_inliers)\n",
    "    df_fasttext = pd.read_csv(log_locator.get_fasttext_csv_path())\n",
    "    fastText_bin_model_2_word2vec(logfile_cfg, best_hyperparams, df_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b68e739e79a7d341429bfa61dd7961b27d5831527a11ca2bf8d991c396b6be9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
